{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdBerg21/AI-Professional-Prompts/blob/main/IMPROVE_Qwen_4B_prompt_engineering_ipynb_(1)_txt_ipynb_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Jl7GD-MzXgrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbCdVyKIMOW",
        "outputId": "db39e455-1e46-4ead-eb3c-2be4336c96c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.35.2\n",
            "Uninstalling transformers-4.35.2:\n",
            "  Successfully uninstalled transformers-4.35.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbDqosI6IYHR",
        "outputId": "42b697ae-bfb3-47d8-df2f-ad955c582ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.37.0\n",
            "  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.0) (2024.2.2)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.37.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.37.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zti56UkIsvN",
        "outputId": "841310e6-2a1c-4869-9de4-acb595832a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/279.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m276.5/279.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWxsgySoZJ3s",
        "outputId": "5951e9fc-6eea-45c0-f294-eb4fd159054b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSJc6UnYZUNf",
        "outputId": "d9366985-a8aa-4a68-f005-2309d48b9dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers_stream_generator\n",
            "  Downloading transformers-stream-generator-0.0.4.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.37.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.26.1->transformers_stream_generator) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.26.1->transformers_stream_generator) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2024.2.2)\n",
            "Building wheels for collected packages: transformers_stream_generator\n",
            "  Building wheel for transformers_stream_generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers_stream_generator: filename=transformers_stream_generator-0.0.4-py3-none-any.whl size=12316 sha256=8cee4d705b6e118b69dfbbaa9801ceb868de6991494783f2dbda45098ea0e824\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/1d/3c/92d88493ed40c0d9be60a391eb76c9a56e9f9b7542cb789401\n",
            "Successfully built transformers_stream_generator\n",
            "Installing collected packages: transformers_stream_generator\n",
            "Successfully installed transformers_stream_generator-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers_stream_generator einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7-IADmicdtM",
        "outputId": "392a18d6-9b88-4ecf-9583-67b20b5887ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers_stream_generator in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: transformers>=4.26.1 in /usr/local/lib/python3.10/dist-packages (from transformers_stream_generator) (4.37.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.26.1->transformers_stream_generator) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.26.1->transformers_stream_generator) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.26.1->transformers_stream_generator) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.26.1->transformers_stream_generator) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate"
      ],
      "metadata": {
        "id": "AmsS5IcoZf_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovkUlecLHulO",
        "outputId": "80d44e0c-9e1c-4418-fb8d-3efb138aa27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1408: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Give me a short introduction to large language models.\n",
            "assistant\n",
            "Large Language Models (LLMs) are complex artificial intelligence models that have achieved impressive levels of performance in various natural language processing tasks, such as text generation, machine translation, question answering, and chatbots. These models are based on deep learning algorithms that enable them to learn from vast amounts of data, recognize patterns, and generate human-like responses to input text.\n",
            "\n",
            "Here's an overview of how LLMs work:\n",
            "\n",
            "1. Data Collection: The first step in building an LLM is collecting a large corpus of text data, which consists of text samples from various sources like books, articles, social media posts, websites, and other languages. This dataset serves as the foundation for training the model by exposing it to different linguistic structures, grammar rules, and vocabulary.\n",
            "\n",
            "2. Preprocessing: The collected data is often cleaned, tokenized, and transformed into a format suitable for training LLMs. Tokenization involves breaking down words into individual units called tokens, usually consisting of a prefix, a base word, and its ending part. This process enables the model to understand the context of each sentence or phrase.\n",
            "\n",
            "3. Training: The preprocessed data is then fed into a deep neural network architecture known as a transformer model. Transformers are self-attention mechanisms that operate at multiple levels of abstraction, allowing the model to focus on specific aspects of the input text without being overwhelmed by irrelevant information. The core component of a transformer is a sequence-to-sequence (Seq2Seq) architecture, where one encoder layer processes the input tokens, and another decoder layer generates output sequences. By adjusting the weights between these layers, the model learns to generate coherent and relevant text based on the given input.\n",
            "\n",
            "4. Fine-tuning: Once trained, the LLM is fine-tuned using a separate task, such as generating summaries, correcting errors, or translating text from one language to another. During fine-tuning, the model is exposed to a smaller subset of the original dataset while maintaining its previous model architecture and parameters. This allows the model to adapt to the specific requirements of the target task more efficiently.\n",
            "\n",
            "5. Evaluation: To evaluate the effectiveness of the LLM, several evaluation metrics are used, such as perplexity, BLEU score, ROUGE-L, and F1-score. Perplexity measures how well the generated text resembles the reference text, while BLEU scores account for the overlap between the predicted and actual sentences in terms of fluency and coherence. ROUGE-L and F1-score are commonly used to quantify the quality of the generated text in terms of semantic\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set the device (e.g., 'cuda' for GPU or 'cpu' for CPU)\n",
        "#device = \"cuda\"\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\", device_map=\"auto\", trust_remote_code=True )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")\n",
        "\n",
        "# Define your prompt\n",
        "prompt = \"Give me a short introduction to large language models.\"\n",
        "\n",
        "# Create a list of messages (system message and user message)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# Apply chat template to generate text\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the text and prepare model inputs\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "# Generate content using the model\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzAkTG_oXz9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d9dda8-067d-4008-e882-efef33d20eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1408: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Give me a short introduction to large language models.\n",
            "assistant\n",
            "Large language models (LLMs) are artificial intelligence models that have been designed to generate human-like text, speech, or code based on input data. They are powered by deep learning algorithms that enable them to learn vast amounts of data and automatically improve their ability to understand, create, and produce natural language over time.\n",
            "These models typically use a type of neural network called a transformer, which is specifically designed for sequence-to-sequence tasks such as machine translation, summarization, question-answering, and chatbot development. The transformer architecture consists of multiple layers of interconnected nodes, each responsible for processing different aspects of the input data, such as word embeddings, positional encoding, attention mechanisms, and hidden states. This allows LLMs to capture complex dependencies between words and phrases in the input and generate coherent and meaningful responses.\n",
            "The training process for an LLM involves feeding it large amounts of text data, along with corresponding labels or feedback from human experts. The model then learns to identify patterns and relationships within the data, adjusting its internal representations and parameters to minimize errors and maximize performance on a specific task. This process can take many iterations, often requiring hundreds or even millions of training examples, to achieve high levels of accuracy and fluency.\n",
            "One key advantage of LLMs is their ability to generate a wide range of natural-sounding text across various domains and languages, including formal and informal language, poetry, fiction, news articles, product descriptions, scientific papers, and more. They have also shown impressive capabilities in generating text with coherence, context awareness, and diversity, making them powerful tools for a variety of applications such as content generation, chatbots, sentiment analysis, and language translation.\n",
            "However, there are also challenges associated with building and deploying large language models, including computational complexity, ethical considerations, privacy concerns, and interpretability issues. As these models become more prevalent in everyday life, they will require careful regulation, evaluation, and oversight to ensure their safe and responsible use while preserving user trust and ensuring fair competition among providers.\n"
          ]
        }
      ],
      "source": [
        "# Define your prompt\n",
        "prompt = \"Give me a short introduction to large language models.\"\n",
        "\n",
        "# Create a list of messages (system message and user message)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# Apply chat template to generate text\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the text and prepare model inputs\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "# Generate content using the model\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DINrkuJVfJHM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axKeX9Gte_5n"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHoN8WumYs5q"
      },
      "outputs": [],
      "source": [
        "# Define your prompt\n",
        "prompt = \"Show how to use a RAG method to query a. pdf file using Qwen model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdaS-mJCeoB3"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text and prepare model inputs\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm38a0EsefNM"
      },
      "outputs": [],
      "source": [
        "# Decode the generated output\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoGtonJlYuik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfba479-f2e6-4c75-8af2-55f5bc85d4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Give me a short introduction to large language models.\n",
            "assistant\n",
            "Large language models (LLMs) are artificial intelligence models that have been designed to generate human-like text, speech, or code based on input data. They are powered by deep learning algorithms that enable them to learn vast amounts of data and automatically improve their ability to understand, create, and produce natural language over time.\n",
            "These models typically use a type of neural network called a transformer, which is specifically designed for sequence-to-sequence tasks such as machine translation, summarization, question-answering, and chatbot development. The transformer architecture consists of multiple layers of interconnected nodes, each responsible for processing different aspects of the input data, such as word embeddings, positional encoding, attention mechanisms, and hidden states. This allows LLMs to capture complex dependencies between words and phrases in the input and generate coherent and meaningful responses.\n",
            "The training process for an LLM involves feeding it large amounts of text data, along with corresponding labels or feedback from human experts. The model then learns to identify patterns and relationships within the data, adjusting its internal representations and parameters to minimize errors and maximize performance on a specific task. This process can take many iterations, often requiring hundreds or even millions of training examples, to achieve high levels of accuracy and fluency.\n",
            "One key advantage of LLMs is their ability to generate a wide range of natural-sounding text across various domains and languages, including formal and informal language, poetry, fiction, news articles, product descriptions, scientific papers, and more. They have also shown impressive capabilities in generating text with coherence, context awareness, and diversity, making them powerful tools for a variety of applications such as content generation, chatbots, sentiment analysis, and language translation.\n",
            "However, there are also challenges associated with building and deploying large language models, including computational complexity, ethical considerations, privacy concerns, and interpretability issues. As these models become more prevalent in everyday life, they will require careful regulation, evaluation, and oversight to ensure their safe and responsible use while preserving user trust and ensuring fair competition among providers.\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhMKcpCdiIHH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define your prompt\n",
        "prompt = \"Write a python code to use a RAG method to query a. pdf file using Qwen model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhD-KdDficBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b76f26-24bd-4677-a262-f05d507846c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUJn88duidV6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenize the text and prepare model inputs\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfdWVIViiYIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c223765-6e99-4c08-8b4b-a34d6189e80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Give me a short introduction to large language models.\n",
            "assistant\n",
            "Large language models (LLMs) are artificial intelligence models that have been designed to generate human-like text, speech, or code based on input data. They are powered by deep learning algorithms that enable them to learn vast amounts of data and automatically improve their ability to understand, create, and produce natural language over time.\n",
            "These models typically use a type of neural network called a transformer, which is specifically designed for sequence-to-sequence tasks such as machine translation, summarization, question-answering, and chatbot development. The transformer architecture consists of multiple layers of interconnected nodes, each responsible for processing different aspects of the input data, such as word embeddings, positional encoding, attention mechanisms, and hidden states. This allows LLMs to capture complex dependencies between words and phrases in the input and generate coherent and meaningful responses.\n",
            "The training process for an LLM involves feeding it large amounts of text data, along with corresponding labels or feedback from human experts. The model then learns to identify patterns and relationships within the data, adjusting its internal representations and parameters to minimize errors and maximize performance on a specific task. This process can take many iterations, often requiring hundreds or even millions of training examples, to achieve high levels of accuracy and fluency.\n",
            "One key advantage of LLMs is their ability to generate a wide range of natural-sounding text across various domains and languages, including formal and informal language, poetry, fiction, news articles, product descriptions, scientific papers, and more. They have also shown impressive capabilities in generating text with coherence, context awareness, and diversity, making them powerful tools for a variety of applications such as content generation, chatbots, sentiment analysis, and language translation.\n",
            "However, there are also challenges associated with building and deploying large language models, including computational complexity, ethical considerations, privacy concerns, and interpretability issues. As these models become more prevalent in everyday life, they will require careful regulation, evaluation, and oversight to ensure their safe and responsible use while preserving user trust and ensuring fair competition among providers.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Set the device (e.g., 'cuda' for GPU or 'cpu' for CPU)\n",
        "#device = \"cuda\"\n",
        "\n",
        "# Load the pretrained model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\", device_map=\"auto\", trust_remote_code=True )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")\n",
        "\n",
        "# Define your prompt\n",
        "prompt = \"Write a python code to use a RAG method to query a. pdf file using Qwen model.\"\n",
        "\n",
        "# Create a list of messages (system message and user message)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# Apply chat template to generate text\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Tokenize the text and prepare model inputs\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "# Generate content using the model\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=51200\n",
        ")\n",
        "\n",
        "# Decode the generated output\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob6bHeSgo7Rz",
        "outputId": "49b44d43-ad47-4a51-d925-fd9c6a3ad02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "system\n",
            "You are a helpful assistant.\n",
            "user\n",
            "Write a python code to use a RAG method to query a. pdf file using Qwen model.\n",
            "assistant\n",
            "The Rag method is a feature extraction technique used in computer vision and machine learning for document-level visual recognition tasks, such as identifying named entities, recognizing faces, and detecting objects in images. Here's an example Python code that uses the PyQwen library to query a PDF file using the Rag method:\n",
            "```python\n",
            "from pyqwen import QWEngine\n",
            "\n",
            "# Load the PDF file into a PyQwen engine\n",
            "engine = QWEngine('file.pdf')\n",
            "\n",
            "# Extract features from the PDF using the RAG method\n",
            "features = engine.rag()\n",
            "\n",
            "# Print the extracted features\n",
            "for feature in features:\n",
            "    print(feature)\n",
            "```\n",
            "\n",
            "In this code, we first load the PDF file into a `QWEngine` object using the `QWEngine` constructor with the path of the PDF file. The `rag()` method is then called on the engine, which returns a dictionary containing various Rag features for the PDF, including:\n",
            "\n",
            "  * `tokens`: A list of tokens representing the text content of the PDF.\n",
            "  * `tags`: A dictionary containing tags representing the objects or concepts present in the PDF (e.g., person, organization, location).\n",
            "  * `entities`: A list of entity IDs (e.g., 'PERSON', 'ORGANIZATION') associated with each token.\n",
            "\n",
            "We can access these features by iterating over the returned dictionary and printing them one by one. Each feature dictionary contains several sub-dictionaries, including:\n",
            "\n",
            "  * `token`: A dictionary containing information about the token, such as its type, position, and language.\n",
            "  * `tag`: A dictionary containing information about the tag associated with the token, such as its name, part-of-speech, and category.\n",
            "  * `entity`: A dictionary containing information about the entity ID associated with the token, such as its name, description, and type.\n",
            "\n",
            "For example, consider the following input PDF file:\n",
            "\n",
            "```\n",
            "This is a sample PDF file with some text content.\n",
            "John Doe works at Google.\n",
            "The company was founded in 2004.\n",
            "```\n",
            "\n",
            "The output of the above code would be:\n",
            "\n",
            "```\n",
            "{\n",
            "    'token': {'type': 'PUNCT', 'position': [0], 'language': 'en-US'},\n",
            "    'tag': {'name': 'PER', 'part_of_speech': 'NOUN', 'category': 'LOCATION'},\n",
            "    'entity': {'name': 'JOHNS DAWE', 'description': 'PERSON', 'type': 'PERSON'}\n",
            "}\n",
            "```\n",
            "\n",
            "Here, the `token` dictionary contains the\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}