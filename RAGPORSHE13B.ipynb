{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4FHDP3g81779VML1wy7A7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdBerg21/AI-Professional-Prompts/blob/main/RAGPORSHE13B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INt4h_NkxUxy"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
        "!pip install langchain einops accelerate transformers bitsandbytes scipy\n",
        "!pip install xformers sentencepiece\n",
        "!pip install llama-index llama_hub --upgrade\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf2\n",
        "!pip install git+https://github.com/huggingface/transformers.git@main --quiet\n",
        "!pip install git+https://github.com/huggingface/accelerate@main --quiet\n",
        "!pip install tensor_parallel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context, Document\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "4B55DxNbxV_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface"
      ],
      "metadata": {
        "id": "2DGS_I_Sxbbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV into a Pandas DataFrame\n",
        "df = pd.read_csv(\"/content/porsche_911.csv\")\n",
        "\n",
        "# Convert the DataFrame content into a format suitable for Llama\n",
        "documents = [\n",
        "    Document(\n",
        "        text=\" \".join([f\"{col}: {value}\" for col, value in zip(df.columns, row.astype(str))]),\n",
        "        metadata={\"row_num\": idx}\n",
        "    )\n",
        "    for idx, row in df.iterrows()\n",
        "]"
      ],
      "metadata": {
        "id": "-wchybuUxe7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama setup\n",
        "\n",
        "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token= \"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\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto',torch_dtype=torch.float16 ,  token= \"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\")"
      ],
      "metadata": {
        "id": "hSgAHmnkxiQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Your goal is to provide answers relating to the car Porsche from the csv.<</SYS>>\"\"\"\n",
        "\n",
        "query_wrapper_prompt = \"{query_str}\"\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "embeddings = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
        "service_context = ServiceContext.from_defaults(chunk_size=4098, llm=llm, embed_model=embeddings)\n",
        "set_global_service_context(service_context)\n",
        "\n",
        "# Create an index using the DataFrame's content\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "def generate_response(query_text):\n",
        "    input_tokens = tokenizer(query_text, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    output = model.generate(**input_tokens)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "9nC8_jAdxmku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Queries\n",
        "queries = [\n",
        "    \"Describe the fuel efficiency trends in Porsche 911 models from 2010 to 2020.\",\n",
        "    \"Which models of the Porsche 911 have a rear-wheel-drive powertrain architecture?\",\n",
        "    \"What is the most relevant year and why for the Porsche 911 dataset?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Response: {generate_response(query)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "I_73quPTxri3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}