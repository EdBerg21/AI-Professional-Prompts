{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdBerg21/AI-Professional-Prompts/blob/main/CONSEGUI_QUASETest_of_%C4%B4ais_01_rag_using_llama_2_langchain_and_chromadb_ipynb_txt_ipynb_txt_ipynb_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc160f60",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "papermill": {
          "duration": 0.009055,
          "end_time": "2023-10-27T20:56:19.591480",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.582425",
          "status": "completed"
        },
        "tags": [],
        "id": "dc160f60"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n",
        "\n",
        "## Objective\n",
        "\n",
        "Use Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\n",
        "When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed.\n",
        "\n",
        "## Definitions\n",
        "\n",
        "* LLM - Large Language Model  \n",
        "* Llama 2.0 - LLM from Meta\n",
        "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
        "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
        "* ChromaDB - vector database  \n",
        "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
        "\n",
        "## Model details\n",
        "\n",
        "* **Model**: Llama 2  \n",
        "* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n",
        "* **Version**: V1  \n",
        "* **Framework**: PyTorch  \n",
        "\n",
        "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n",
        "\n",
        "\n",
        "## What is a Retrieval Augmented Generation (RAG) system?\n",
        "\n",
        "Large Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n",
        "\n",
        "The retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n",
        "\n",
        "For the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n",
        "\n",
        "The orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n",
        "\n",
        "## More about this  \n",
        "\n",
        "Do you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deea9c35",
      "metadata": {
        "papermill": {
          "duration": 0.008483,
          "end_time": "2023-10-27T20:56:19.608601",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.600118",
          "status": "completed"
        },
        "tags": [],
        "id": "deea9c35"
      },
      "source": [
        "# Installations, imports, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4f633640",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:56:19.627014Z",
          "iopub.status.busy": "2023-10-27T20:56:19.626657Z",
          "iopub.status.idle": "2023-10-27T20:59:21.477666Z",
          "shell.execute_reply": "2023-10-27T20:59:21.476526Z"
        },
        "papermill": {
          "duration": 181.863041,
          "end_time": "2023-10-27T20:59:21.480141",
          "exception": false,
          "start_time": "2023-10-27T20:56:19.617100",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f633640",
        "outputId": "40da4e86-9a5a-4f57-aa21-f74c97706ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.35.2\n",
            "Uninstalling transformers-4.35.2:\n",
            "  Successfully uninstalled transformers-4.35.2\n",
            "Found existing installation: accelerate 0.22.0\n",
            "Uninstalling accelerate-0.22.0:\n",
            "  Successfully uninstalled accelerate-0.22.0\n",
            "\u001b[33mWARNING: Skipping einops as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: bitsandbytes 0.41.1\n",
            "Uninstalling bitsandbytes-0.41.1:\n",
            "  Successfully uninstalled bitsandbytes-0.41.1\n",
            "\u001b[33mWARNING: Skipping sentence_transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping chromadb as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
        "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes==0.41.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59xx8w-h2V5W",
        "outputId": "d761e580-d56c-4329-dc82-03d6b92c74bf"
      },
      "id": "59xx8w-h2V5W",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes==0.41.1\n",
            "  Using cached bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.22.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28YGRv9k20aG",
        "outputId": "4c1b86e2-2630-4433-f302-9e660b9e67d1"
      },
      "id": "28YGRv9k20aG",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.22.0\n",
            "  Using cached accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.22.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.22.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.22.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.22.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.22.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb==0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "epQaMtCxE7p-",
        "outputId": "bda35fb9-e64f-41d7-a93e-7a95c0d7f648"
      },
      "id": "epQaMtCxE7p-",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb==0.4.12\n",
            "  Using cached chromadb-0.4.12-py3-none-any.whl (426 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (2.31.0)\n",
            "Collecting pydantic<2.0,>=1.9 (from chromadb==0.4.12)\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.12)\n",
            "  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.4.12)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb==0.4.12)\n",
            "  Downloading posthog-3.4.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.4.12)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.4.12)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.12)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (6.1.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.12)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.4.12) (1.25.2)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.12)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.12)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.12)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.12) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.4.12) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.12) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.4.12)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.12)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.12)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.4.12)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.12)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.4.12)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.12) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.12) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (1.2.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=f53bdc0a1b50859b430d3f675aff186cdd7efe8e5c9bc6d21f1ab2d976f64784\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, python-dotenv, pydantic, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.1\n",
            "    Uninstalling pydantic-2.6.1:\n",
            "      Successfully uninstalled pydantic-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.12 coloredlogs-15.0.1 fastapi-0.99.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.17.0 overrides-7.7.0 posthog-3.4.1 pulsar-client-3.4.0 pydantic-1.10.14 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.27.0 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydantic"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.33.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8FfEV_DF43Z",
        "outputId": "5f7c3760-13a2-40c0-b008-cca1edd8c309"
      },
      "id": "G8FfEV_DF43Z",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.33.0\n",
            "  Using cached transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.0) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2W2G56VGHwJ",
        "outputId": "11288c65-9aa1-4e86-fccc-b1e1eafc271d"
      },
      "id": "k2W2G56VGHwJ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.300\n",
            "  Using cached langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (3.9.3)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.300)\n",
            "  Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.300)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n",
            "  Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (2.9.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.300) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.300) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.300) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m822.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.300)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.300) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.300) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.300) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.300) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (3.0.3)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.300 langsmith-0.0.92 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo_qEKblJqAA",
        "outputId": "7a500720-6e74-4722-efb3-1f4a21aa81cb"
      },
      "id": "xo_qEKblJqAA",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers==2.2.2\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.33.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.2.2) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=d67d33abf2562432debbd309f71b2a003510a3728d45c332e68f4a4424528606\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9cd3737d",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:59:21.624603Z",
          "iopub.status.busy": "2023-10-27T20:59:21.624219Z",
          "iopub.status.idle": "2023-10-27T20:59:29.347292Z",
          "shell.execute_reply": "2023-10-27T20:59:29.346135Z"
        },
        "papermill": {
          "duration": 7.79784,
          "end_time": "2023-10-27T20:59:29.350209",
          "exception": false,
          "start_time": "2023-10-27T20:59:21.552369",
          "status": "completed"
        },
        "tags": [],
        "id": "9cd3737d"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc807934",
      "metadata": {
        "papermill": {
          "duration": 0.069963,
          "end_time": "2023-10-27T20:59:29.491611",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.421648",
          "status": "completed"
        },
        "tags": [],
        "id": "dc807934"
      },
      "source": [
        "# Initialize model, tokenizer, query pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac27581",
      "metadata": {
        "papermill": {
          "duration": 0.070033,
          "end_time": "2023-10-27T20:59:29.632562",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.562529",
          "status": "completed"
        },
        "tags": [],
        "id": "cac27581"
      },
      "source": [
        "Define the model, the device, and the `bitsandbytes` configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "token=\"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\""
      ],
      "metadata": {
        "id": "-h8asI675-WP"
      },
      "id": "-h8asI675-WP"
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token 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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHdo5jIH0I2g",
        "outputId": "fa886e35-443c-409c-b40b-bfd7b6412af3"
      },
      "id": "DHdo5jIH0I2g",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "04c42ecd",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:59:29.774721Z",
          "iopub.status.busy": "2023-10-27T20:59:29.773919Z",
          "iopub.status.idle": "2023-10-27T20:59:29.864243Z",
          "shell.execute_reply": "2023-10-27T20:59:29.863458Z"
        },
        "papermill": {
          "duration": 0.163665,
          "end_time": "2023-10-27T20:59:29.866257",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.702592",
          "status": "completed"
        },
        "tags": [],
        "id": "04c42ecd"
      },
      "outputs": [],
      "source": [
        "model_id = 'inception-mbzuai/jais-13b-chat'\n",
        "\n",
        "#device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33296d38",
      "metadata": {
        "papermill": {
          "duration": 0.070293,
          "end_time": "2023-10-27T20:59:30.007969",
          "exception": false,
          "start_time": "2023-10-27T20:59:29.937676",
          "status": "completed"
        },
        "tags": [],
        "id": "33296d38"
      },
      "source": [
        "Prepare the model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "489fbbf7",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T20:59:30.152371Z",
          "iopub.status.busy": "2023-10-27T20:59:30.151551Z",
          "iopub.status.idle": "2023-10-27T21:02:57.429735Z",
          "shell.execute_reply": "2023-10-27T21:02:57.428570Z"
        },
        "papermill": {
          "duration": 207.351985,
          "end_time": "2023-10-27T21:02:57.431750",
          "exception": false,
          "start_time": "2023-10-27T20:59:30.079765",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "624bbc441cb44089ace2c77ac48ebb47",
            "2cf636bb21a04631a08a5cc109e89ba5",
            "ac9f4d0d917745eb8a2a733e9452f6bf",
            "d0d4d98d8afc4120b60a5cb2ce2e8990",
            "147c09c4369d4248ba9888af84a781bb",
            "90fd154966cf45748d5222105e8b76a2",
            "daa983c9b1b84a9987b096d529e89a10",
            "1f83e917d0004708a56315b61c9af73c",
            "1697592311174efc8222ce47230198ca",
            "395f296445ac4a6d8dbdb59f3275857e",
            "a8ab9526e8904e9ab709eb22a09b7ae5"
          ]
        },
        "id": "489fbbf7",
        "outputId": "e36543a7-3021-4e79-e3bb-89565c3b12c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "624bbc441cb44089ace2c77ac48ebb47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare model, tokenizer: 295.364 sec.\n"
          ]
        }
      ],
      "source": [
        "time_1 = time()\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    \"inception-mbzuai/jais-13b-chat\",\n",
        "    trust_remote_code = True\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code = True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "time_2 = time()\n",
        "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c5a616",
      "metadata": {
        "papermill": {
          "duration": 0.071483,
          "end_time": "2023-10-27T21:02:57.575572",
          "exception": false,
          "start_time": "2023-10-27T21:02:57.504089",
          "status": "completed"
        },
        "tags": [],
        "id": "21c5a616"
      },
      "source": [
        "Define the query pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6d4257",
      "metadata": {
        "papermill": {
          "duration": 0.07169,
          "end_time": "2023-10-27T21:02:59.639272",
          "exception": false,
          "start_time": "2023-10-27T21:02:59.567582",
          "status": "completed"
        },
        "tags": [],
        "id": "0e6d4257"
      },
      "source": [
        "We define a function for testing the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "43f4b1c6",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:02:59.783315Z",
          "iopub.status.busy": "2023-10-27T21:02:59.782571Z",
          "iopub.status.idle": "2023-10-27T21:02:59.788716Z",
          "shell.execute_reply": "2023-10-27T21:02:59.787817Z"
        },
        "papermill": {
          "duration": 0.080139,
          "end_time": "2023-10-27T21:02:59.790639",
          "exception": false,
          "start_time": "2023-10-27T21:02:59.710500",
          "status": "completed"
        },
        "tags": [],
        "id": "43f4b1c6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_model(tokenizer, pipeline, prompt_to_test):\n",
        "    \"\"\"\n",
        "    Perform a query\n",
        "    print the result\n",
        "    Args:\n",
        "        tokenizer: the tokenizer\n",
        "        pipeline: the pipeline\n",
        "        prompt_to_test: the prompt\n",
        "    Returns\n",
        "        None\n",
        "    \"\"\"\n",
        "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
        "    time_1 = time()\n",
        "    sequences = pipeline(\n",
        "        prompt_to_test,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=500,)\n",
        "    time_2 = time()\n",
        "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
        "    for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2444aca8",
      "metadata": {
        "papermill": {
          "duration": 0.070941,
          "end_time": "2023-10-27T21:02:59.932300",
          "exception": false,
          "start_time": "2023-10-27T21:02:59.861359",
          "status": "completed"
        },
        "tags": [],
        "id": "2444aca8"
      },
      "source": [
        "## Test the query pipeline\n",
        "\n",
        "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "vYpaJ24jFcBU"
      },
      "id": "vYpaJ24jFcBU",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_1 = time()\n",
        "query_pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "time_2 = time()\n",
        "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWs61rTsNCon",
        "outputId": "2ac0a486-71b4-4384-d9bd-4176ed349bae"
      },
      "id": "pWs61rTsNCon",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare pipeline: 0.0 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "8c565a85",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:00.077578Z",
          "iopub.status.busy": "2023-10-27T21:03:00.076738Z",
          "iopub.status.idle": "2023-10-27T21:03:07.197627Z",
          "shell.execute_reply": "2023-10-27T21:03:07.196596Z"
        },
        "papermill": {
          "duration": 7.196414,
          "end_time": "2023-10-27T21:03:07.199767",
          "exception": false,
          "start_time": "2023-10-27T21:03:00.003353",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c565a85",
        "outputId": "e9e5c9e8-be81-4648-f1dd-2f055e517c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test inference: 1.197 sec.\n",
            "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\n"
          ]
        }
      ],
      "source": [
        "test_model(tokenizer,\n",
        "           query_pipeline,\n",
        "           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8afac76",
      "metadata": {
        "papermill": {
          "duration": 0.0722,
          "end_time": "2023-10-27T21:03:07.344434",
          "exception": false,
          "start_time": "2023-10-27T21:03:07.272234",
          "status": "completed"
        },
        "tags": [],
        "id": "d8afac76"
      },
      "source": [
        "# Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8569323",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
          "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
          "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
          "shell.execute_reply": "2023-09-23T19:22:16.439217Z",
          "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z"
        },
        "papermill": {
          "duration": 0.071285,
          "end_time": "2023-10-27T21:03:07.488216",
          "exception": false,
          "start_time": "2023-10-27T21:03:07.416931",
          "status": "completed"
        },
        "tags": [],
        "id": "a8569323"
      },
      "source": [
        "## Check the model with a HuggingFace pipeline\n",
        "\n",
        "\n",
        "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a55c51a2",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:07.632834Z",
          "iopub.status.busy": "2023-10-27T21:03:07.632222Z",
          "iopub.status.idle": "2023-10-27T21:03:12.199600Z",
          "shell.execute_reply": "2023-10-27T21:03:12.198697Z"
        },
        "papermill": {
          "duration": 4.641707,
          "end_time": "2023-10-27T21:03:12.201731",
          "exception": false,
          "start_time": "2023-10-27T21:03:07.560024",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a55c51a2",
        "outputId": "fc443134-790b-4fc9-9c63-b351dcb8dc91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8951a9e",
      "metadata": {
        "papermill": {
          "duration": 0.071941,
          "end_time": "2023-10-27T21:03:12.347303",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.275362",
          "status": "completed"
        },
        "tags": [],
        "id": "f8951a9e"
      },
      "source": [
        "## Ingestion of data using Text loder\n",
        "\n",
        "We will ingest the newest presidential address, from Jan 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "f45938da",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:12.492486Z",
          "iopub.status.busy": "2023-10-27T21:03:12.491830Z",
          "iopub.status.idle": "2023-10-27T21:03:12.510111Z",
          "shell.execute_reply": "2023-10-27T21:03:12.509378Z"
        },
        "papermill": {
          "duration": 0.092791,
          "end_time": "2023-10-27T21:03:12.511927",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.419136",
          "status": "completed"
        },
        "tags": [],
        "id": "f45938da"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"/content/biden-sotu-2023-autogenerated-transcript.txt\",\n",
        "                    encoding=\"utf8\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31db8500",
      "metadata": {
        "papermill": {
          "duration": 0.071305,
          "end_time": "2023-10-27T21:03:12.654717",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.583412",
          "status": "completed"
        },
        "tags": [],
        "id": "31db8500"
      },
      "source": [
        "## Split data in chunks\n",
        "\n",
        "We split data in chunks using a recursive character text splitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "3636cbf3",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:12.799624Z",
          "iopub.status.busy": "2023-10-27T21:03:12.798854Z",
          "iopub.status.idle": "2023-10-27T21:03:12.826146Z",
          "shell.execute_reply": "2023-10-27T21:03:12.825120Z"
        },
        "papermill": {
          "duration": 0.102636,
          "end_time": "2023-10-27T21:03:12.828350",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.725714",
          "status": "completed"
        },
        "tags": [],
        "id": "3636cbf3"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddc41386",
      "metadata": {
        "papermill": {
          "duration": 0.071654,
          "end_time": "2023-10-27T21:03:12.973609",
          "exception": false,
          "start_time": "2023-10-27T21:03:12.901955",
          "status": "completed"
        },
        "tags": [],
        "id": "ddc41386"
      },
      "source": [
        "## Creating Embeddings and Storing in Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1cef9c",
      "metadata": {
        "papermill": {
          "duration": 0.072129,
          "end_time": "2023-10-27T21:03:13.117787",
          "exception": false,
          "start_time": "2023-10-27T21:03:13.045658",
          "status": "completed"
        },
        "tags": [],
        "id": "8b1cef9c"
      },
      "source": [
        "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a8fe608f",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:13.262680Z",
          "iopub.status.busy": "2023-10-27T21:03:13.261674Z",
          "iopub.status.idle": "2023-10-27T21:03:20.269116Z",
          "shell.execute_reply": "2023-10-27T21:03:20.268254Z"
        },
        "papermill": {
          "duration": 7.082713,
          "end_time": "2023-10-27T21:03:20.271413",
          "exception": false,
          "start_time": "2023-10-27T21:03:13.188700",
          "status": "completed"
        },
        "tags": [],
        "id": "a8fe608f"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0163fe3",
      "metadata": {
        "papermill": {
          "duration": 0.07346,
          "end_time": "2023-10-27T21:03:20.419320",
          "exception": false,
          "start_time": "2023-10-27T21:03:20.345860",
          "status": "completed"
        },
        "tags": [],
        "id": "e0163fe3"
      },
      "source": [
        "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "d44d4386",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:20.569088Z",
          "iopub.status.busy": "2023-10-27T21:03:20.568404Z",
          "iopub.status.idle": "2023-10-27T21:03:21.695319Z",
          "shell.execute_reply": "2023-10-27T21:03:21.694514Z"
        },
        "papermill": {
          "duration": 1.204428,
          "end_time": "2023-10-27T21:03:21.697360",
          "exception": false,
          "start_time": "2023-10-27T21:03:20.492932",
          "status": "completed"
        },
        "tags": [],
        "id": "d44d4386"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede70a2d",
      "metadata": {
        "papermill": {
          "duration": 0.073547,
          "end_time": "2023-10-27T21:03:21.846037",
          "exception": false,
          "start_time": "2023-10-27T21:03:21.772490",
          "status": "completed"
        },
        "tags": [],
        "id": "ede70a2d"
      },
      "source": [
        "## Initialize chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "12a89658",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:21.995328Z",
          "iopub.status.busy": "2023-10-27T21:03:21.994982Z",
          "iopub.status.idle": "2023-10-27T21:03:22.000248Z",
          "shell.execute_reply": "2023-10-27T21:03:21.999404Z"
        },
        "papermill": {
          "duration": 0.082483,
          "end_time": "2023-10-27T21:03:22.002212",
          "exception": false,
          "start_time": "2023-10-27T21:03:21.919729",
          "status": "completed"
        },
        "tags": [],
        "id": "12a89658"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e171a1d",
      "metadata": {
        "papermill": {
          "duration": 0.075491,
          "end_time": "2023-10-27T21:03:22.151652",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.076161",
          "status": "completed"
        },
        "tags": [],
        "id": "1e171a1d"
      },
      "source": [
        "## Test the Retrieval-Augmented Generation\n",
        "\n",
        "\n",
        "We define a test function, that will run the query and time it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "2863b132",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:22.303074Z",
          "iopub.status.busy": "2023-10-27T21:03:22.302237Z",
          "iopub.status.idle": "2023-10-27T21:03:22.307653Z",
          "shell.execute_reply": "2023-10-27T21:03:22.306729Z"
        },
        "papermill": {
          "duration": 0.083507,
          "end_time": "2023-10-27T21:03:22.309806",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.226299",
          "status": "completed"
        },
        "tags": [],
        "id": "2863b132"
      },
      "outputs": [],
      "source": [
        "def test_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    time_1 = time()\n",
        "    result = qa.run(query)\n",
        "    time_2 = time()\n",
        "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
        "    print(\"\\nResult: \", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb9fc42",
      "metadata": {
        "papermill": {
          "duration": 0.074358,
          "end_time": "2023-10-27T21:03:22.460824",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.386466",
          "status": "completed"
        },
        "tags": [],
        "id": "efb9fc42"
      },
      "source": [
        "Let's check few queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "c1b1f2ff",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:22.611036Z",
          "iopub.status.busy": "2023-10-27T21:03:22.610650Z",
          "iopub.status.idle": "2023-10-27T21:03:34.578891Z",
          "shell.execute_reply": "2023-10-27T21:03:34.577954Z"
        },
        "papermill": {
          "duration": 12.045607,
          "end_time": "2023-10-27T21:03:34.581176",
          "exception": false,
          "start_time": "2023-10-27T21:03:22.535569",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1b1f2ff",
        "outputId": "d3b148e9-c66b-41d6-de09-fdcbcb9990c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 1086, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Inference time: 11.129 sec.\n",
            "\n",
            "Result:  \n"
          ]
        }
      ],
      "source": [
        "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
        "test_rag(qa, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "7b764d2f",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:34.730684Z",
          "iopub.status.busy": "2023-10-27T21:03:34.730387Z",
          "iopub.status.idle": "2023-10-27T21:03:45.642067Z",
          "shell.execute_reply": "2023-10-27T21:03:45.641090Z"
        },
        "papermill": {
          "duration": 10.988249,
          "end_time": "2023-10-27T21:03:45.644257",
          "exception": false,
          "start_time": "2023-10-27T21:03:34.656008",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b764d2f",
        "outputId": "400e13f4-4130-46bd-f4c3-f2d4ee725772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 1117, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Inference time: 11.842 sec.\n",
            "\n",
            "Result:  \n"
          ]
        }
      ],
      "source": [
        "query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\n",
        "test_rag(qa, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b8ee84",
      "metadata": {
        "papermill": {
          "duration": 0.076435,
          "end_time": "2023-10-27T21:03:45.796935",
          "exception": false,
          "start_time": "2023-10-27T21:03:45.720500",
          "status": "completed"
        },
        "tags": [],
        "id": "a6b8ee84"
      },
      "source": [
        "## Document sources\n",
        "\n",
        "Let's check the documents sources, for the last query run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "bf90c12d",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-10-27T21:03:45.950514Z",
          "iopub.status.busy": "2023-10-27T21:03:45.950160Z",
          "iopub.status.idle": "2023-10-27T21:03:45.993003Z",
          "shell.execute_reply": "2023-10-27T21:03:45.991579Z"
        },
        "papermill": {
          "duration": 0.122251,
          "end_time": "2023-10-27T21:03:45.994982",
          "exception": false,
          "start_time": "2023-10-27T21:03:45.872731",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf90c12d",
        "outputId": "d9874073-4264-4f44-d7f3-72029ef23f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
            "Retrieved documents: 4\n",
            "Source:  /content/biden-sotu-2023-autogenerated-transcript.txt\n",
            "Text:  to build an economy from the bottom up\n",
            "\n",
            "in the middle out\n",
            "\n",
            "not from the top down because when the\n",
            "\n",
            "middle class does well the poor have a\n",
            "\n",
            "ladder up and the wealthy still do very\n",
            "\n",
            "well we all do well\n",
            "\n",
            "I know a lot of you always kidding me\n",
            "\n",
            "for always quoting my dad but my dad\n",
            "\n",
            "used to say Joey and jobs a bottle a lot\n",
            "\n",
            "more than a paycheck because he really\n",
            "\n",
            "would say this it's about a lot more\n",
            "\n",
            "than a paycheck it's about your dignity\n",
            "\n",
            "it's about respect it's about being able\n",
            "\n",
            "to look your kid in the eye and say\n",
            "\n",
            "honey it's going to be okay and mean it\n",
            "\n",
            "folks\n",
            "\n",
            "so let's look at the results\n",
            "\n",
            "we're not finished yet by any stretch of\n",
            "\n",
            "the imagination but unemployment rate is\n",
            "\n",
            "at 3.4 percent a 50-year low\n",
            "\n",
            "record\n",
            "\n",
            "Premier record unemployment\n",
            "\n",
            "near record unemployment for Black and\n",
            "\n",
            "Hispanic workers we've already created\n",
            "\n",
            "your help eight hundred thousand good\n",
            "\n",
            "paying manufacturing jobs the fastest\n",
            "\n",
            "growth in 40 years\n",
            "\n",
            "and where is it written\n",
            "\n",
            "where is it written that America can't \n",
            "\n",
            "Source:  /content/biden-sotu-2023-autogenerated-transcript.txt\n",
            "Text:  to build an economy from the bottom up\n",
            "\n",
            "in the middle out\n",
            "\n",
            "not from the top down because when the\n",
            "\n",
            "middle class does well the poor have a\n",
            "\n",
            "ladder up and the wealthy still do very\n",
            "\n",
            "well we all do well\n",
            "\n",
            "I know a lot of you always kidding me\n",
            "\n",
            "for always quoting my dad but my dad\n",
            "\n",
            "used to say Joey and jobs a bottle a lot\n",
            "\n",
            "more than a paycheck because he really\n",
            "\n",
            "would say this it's about a lot more\n",
            "\n",
            "than a paycheck it's about your dignity\n",
            "\n",
            "it's about respect it's about being able\n",
            "\n",
            "to look your kid in the eye and say\n",
            "\n",
            "honey it's going to be okay and mean it\n",
            "\n",
            "folks\n",
            "\n",
            "so let's look at the results\n",
            "\n",
            "we're not finished yet by any stretch of\n",
            "\n",
            "the imagination but unemployment rate is\n",
            "\n",
            "at 3.4 percent a 50-year low\n",
            "\n",
            "record\n",
            "\n",
            "Premier record unemployment\n",
            "\n",
            "near record unemployment for Black and\n",
            "\n",
            "Hispanic workers we've already created\n",
            "\n",
            "your help eight hundred thousand good\n",
            "\n",
            "paying manufacturing jobs the fastest\n",
            "\n",
            "growth in 40 years\n",
            "\n",
            "and where is it written\n",
            "\n",
            "where is it written that America can't \n",
            "\n",
            "Source:  /content/biden-sotu-2023-autogenerated-transcript.txt\n",
            "Text:  to build an economy from the bottom up\n",
            "\n",
            "in the middle out\n",
            "\n",
            "not from the top down because when the\n",
            "\n",
            "middle class does well the poor have a\n",
            "\n",
            "ladder up and the wealthy still do very\n",
            "\n",
            "well we all do well\n",
            "\n",
            "I know a lot of you always kidding me\n",
            "\n",
            "for always quoting my dad but my dad\n",
            "\n",
            "used to say Joey and jobs a bottle a lot\n",
            "\n",
            "more than a paycheck because he really\n",
            "\n",
            "would say this it's about a lot more\n",
            "\n",
            "than a paycheck it's about your dignity\n",
            "\n",
            "it's about respect it's about being able\n",
            "\n",
            "to look your kid in the eye and say\n",
            "\n",
            "honey it's going to be okay and mean it\n",
            "\n",
            "folks\n",
            "\n",
            "so let's look at the results\n",
            "\n",
            "we're not finished yet by any stretch of\n",
            "\n",
            "the imagination but unemployment rate is\n",
            "\n",
            "at 3.4 percent a 50-year low\n",
            "\n",
            "record\n",
            "\n",
            "Premier record unemployment\n",
            "\n",
            "near record unemployment for Black and\n",
            "\n",
            "Hispanic workers we've already created\n",
            "\n",
            "your help eight hundred thousand good\n",
            "\n",
            "paying manufacturing jobs the fastest\n",
            "\n",
            "growth in 40 years\n",
            "\n",
            "and where is it written\n",
            "\n",
            "where is it written that America can't \n",
            "\n",
            "Source:  /content/biden-sotu-2023-autogenerated-transcript.txt\n",
            "Text:  to build an economy from the bottom up\n",
            "\n",
            "in the middle out\n",
            "\n",
            "not from the top down because when the\n",
            "\n",
            "middle class does well the poor have a\n",
            "\n",
            "ladder up and the wealthy still do very\n",
            "\n",
            "well we all do well\n",
            "\n",
            "I know a lot of you always kidding me\n",
            "\n",
            "for always quoting my dad but my dad\n",
            "\n",
            "used to say Joey and jobs a bottle a lot\n",
            "\n",
            "more than a paycheck because he really\n",
            "\n",
            "would say this it's about a lot more\n",
            "\n",
            "than a paycheck it's about your dignity\n",
            "\n",
            "it's about respect it's about being able\n",
            "\n",
            "to look your kid in the eye and say\n",
            "\n",
            "honey it's going to be okay and mean it\n",
            "\n",
            "folks\n",
            "\n",
            "so let's look at the results\n",
            "\n",
            "we're not finished yet by any stretch of\n",
            "\n",
            "the imagination but unemployment rate is\n",
            "\n",
            "at 3.4 percent a 50-year low\n",
            "\n",
            "record\n",
            "\n",
            "Premier record unemployment\n",
            "\n",
            "near record unemployment for Black and\n",
            "\n",
            "Hispanic workers we've already created\n",
            "\n",
            "your help eight hundred thousand good\n",
            "\n",
            "paying manufacturing jobs the fastest\n",
            "\n",
            "growth in 40 years\n",
            "\n",
            "and where is it written\n",
            "\n",
            "where is it written that America can't \n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = vectordb.similarity_search(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved documents: {len(docs)}\")\n",
        "for doc in docs:\n",
        "    doc_details = doc.to_json()['kwargs']\n",
        "    print(\"Source: \", doc_details['metadata']['source'])\n",
        "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e752616d",
      "metadata": {
        "papermill": {
          "duration": 0.075988,
          "end_time": "2023-10-27T21:03:46.146601",
          "exception": false,
          "start_time": "2023-10-27T21:03:46.070613",
          "status": "completed"
        },
        "tags": [],
        "id": "e752616d"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "We used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n",
        "\n",
        "\n",
        "# More work on the same topic\n",
        "\n",
        "You can find more details about how to use a LLM with Kaggle. Few interesting topics are treated in:  \n",
        "\n",
        "* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n",
        "* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n",
        "* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)  \n",
        "* https://www.kaggle.com/code/gpreda/explore-enron-emails-with-langchain-and-llama-v2 (Explore Enron Emails with Langchain and Llama v2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ca9f4a",
      "metadata": {
        "papermill": {
          "duration": 0.077721,
          "end_time": "2023-10-27T21:03:46.301029",
          "exception": false,
          "start_time": "2023-10-27T21:03:46.223308",
          "status": "completed"
        },
        "tags": [],
        "id": "71ca9f4a"
      },
      "source": [
        "# References  \n",
        "\n",
        "[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n",
        "\n",
        "[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf\n",
        "\n",
        "[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n",
        "\n",
        "[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n",
        "\n",
        "[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n",
        "\n",
        "[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 453.685483,
      "end_time": "2023-10-27T21:03:49.973763",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-27T20:56:16.288280",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "624bbc441cb44089ace2c77ac48ebb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cf636bb21a04631a08a5cc109e89ba5",
              "IPY_MODEL_ac9f4d0d917745eb8a2a733e9452f6bf",
              "IPY_MODEL_d0d4d98d8afc4120b60a5cb2ce2e8990"
            ],
            "layout": "IPY_MODEL_147c09c4369d4248ba9888af84a781bb"
          }
        },
        "2cf636bb21a04631a08a5cc109e89ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90fd154966cf45748d5222105e8b76a2",
            "placeholder": "​",
            "style": "IPY_MODEL_daa983c9b1b84a9987b096d529e89a10",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ac9f4d0d917745eb8a2a733e9452f6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f83e917d0004708a56315b61c9af73c",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1697592311174efc8222ce47230198ca",
            "value": 6
          }
        },
        "d0d4d98d8afc4120b60a5cb2ce2e8990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_395f296445ac4a6d8dbdb59f3275857e",
            "placeholder": "​",
            "style": "IPY_MODEL_a8ab9526e8904e9ab709eb22a09b7ae5",
            "value": " 6/6 [04:46&lt;00:00, 41.94s/it]"
          }
        },
        "147c09c4369d4248ba9888af84a781bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90fd154966cf45748d5222105e8b76a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daa983c9b1b84a9987b096d529e89a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f83e917d0004708a56315b61c9af73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1697592311174efc8222ce47230198ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "395f296445ac4a6d8dbdb59f3275857e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ab9526e8904e9ab709eb22a09b7ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}